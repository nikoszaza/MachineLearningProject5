{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbdc1ff",
   "metadata": {},
   "source": [
    "# Sentinel Analysis on IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05d77a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout, Conv1D, MaxPooling1D, GRU, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34edf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test and training data.\n"
     ]
    }
   ],
   "source": [
    "# Define the number of words you want to use\n",
    "max_words = 5000\n",
    "\n",
    "# Define the training and test dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "print(\"Created test and training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da56558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded the input sequences with 0's to all be the same length.\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum length of a review\n",
    "max_review_length = 500\n",
    "\n",
    "# Pad the input sequences with 0's to make them all the same length\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(\"Padded the input sequences with 0's to all be the same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8158fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02cf51",
   "metadata": {},
   "source": [
    "We will use scaled data for faster convergence and better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c5562",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c70e4216",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg, C: 0.1, Accuracy: 0.51048\n",
      "Solver: newton-cg, C: 1, Accuracy: 0.50912\n",
      "Solver: newton-cg, C: 10, Accuracy: 0.50888\n",
      "Solver: liblinear, C: 0.1, Accuracy: 0.51016\n",
      "Solver: liblinear, C: 1, Accuracy: 0.50916\n",
      "Solver: liblinear, C: 10, Accuracy: 0.50892\n",
      "Solver: lbfgs, C: 0.1, Accuracy: 0.51052\n",
      "Solver: lbfgs, C: 1, Accuracy: 0.50896\n",
      "Solver: lbfgs, C: 10, Accuracy: 0.50896\n",
      "Solver: sag, C: 0.1, Accuracy: 0.51048\n",
      "Solver: sag, C: 1, Accuracy: 0.50908\n",
      "Solver: sag, C: 10, Accuracy: 0.50884\n",
      "Solver: saga, C: 0.1, Accuracy: 0.51048\n",
      "Solver: saga, C: 1, Accuracy: 0.50912\n",
      "Solver: saga, C: 10, Accuracy: 0.50892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "solvers = ['newton-cg', 'liblinear', 'lbfgs' , 'sag', 'saga']\n",
    "cs = [0.1, 1, 10]\n",
    "\n",
    "\n",
    "for solver in solvers:\n",
    "    for c in cs:\n",
    "        logisticRegr = LogisticRegression(C = c, solver =solver)\n",
    "        logisticRegr.fit(X_train_scaled., y_train)\n",
    "        print(f\"Solver: {solver}, C: {c}, Accuracy: {logisticRegr.score(X_test_scaled, y_test)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0728b",
   "metadata": {},
   "source": [
    "Bad accuracy, almost the same as a random unbiased predictor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227f39d",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cb8fe68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 50.4240%\n",
      "==============================\n",
      "==============================\n",
      "MultinomialNB\n",
      "****Results****\n",
      "Accuracy: 49.9720%\n",
      "==============================\n",
      "==============================\n",
      "BernoulliNB\n",
      "****Results****\n",
      "Accuracy: 50.9400%\n",
      "==============================\n",
      "==============================\n",
      "ComplementNB\n",
      "****Results****\n",
      "Accuracy: 49.9720%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, log_loss, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    ComplementNB(),               \n",
    "                  ]\n",
    " \n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    " \n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, 11]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c46c2",
   "metadata": {},
   "source": [
    "Identical perfomance to random unbiased predictor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286272c",
   "metadata": {},
   "source": [
    "## Decision Tree (Random Forest, Adamboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a7cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2, Max features: sqrt, Accuracy: 0.5264\n",
      "Max Depth: 2, Max features: log2, Accuracy: 0.52172\n",
      "Max Depth: 5, Max features: sqrt, Accuracy: 0.5306\n",
      "Max Depth: 5, Max features: log2, Accuracy: 0.52704\n",
      "Max Depth: 7, Max features: sqrt, Accuracy: 0.53644\n",
      "Max Depth: 7, Max features: log2, Accuracy: 0.531\n",
      "Max Depth: 10, Max features: sqrt, Accuracy: 0.53788\n",
      "Max Depth: 10, Max features: log2, Accuracy: 0.53324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "max_depths = [2, 5, 7, 10]\n",
    "max_features = [\"sqrt\", \"log2\"]\n",
    "\n",
    "# decision tree does not need scaled data to perform better\n",
    "# the same applies to all ensembling methods using desicion tree as base estimator\n",
    "for max_depth in max_depths:\n",
    "    for n_feat in max_features:\n",
    "        rfc = RandomForestClassifier(max_depth = max_depth, max_features = n_feat)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        print(f\"Max Depth: {max_depth}, Max features: {n_feat}, Accuracy: {rfc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b17eaa",
   "metadata": {},
   "source": [
    "Slighty better than a random unbiased predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf48c7b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators: 10, Learning rate: 0.1, Accuracy: 0.52096\n",
      "Number of estimators: 10, Learning rate: 0.5, Accuracy: 0.52648\n",
      "Number of estimators: 10, Learning rate: 1, Accuracy: 0.52624\n",
      "Number of estimators: 50, Learning rate: 0.1, Accuracy: 0.53084\n",
      "Number of estimators: 50, Learning rate: 0.5, Accuracy: 0.54184\n",
      "Number of estimators: 50, Learning rate: 1, Accuracy: 0.5416\n",
      "Number of estimators: 100, Learning rate: 0.1, Accuracy: 0.5388\n",
      "Number of estimators: 100, Learning rate: 0.5, Accuracy: 0.54984\n",
      "Number of estimators: 100, Learning rate: 1, Accuracy: 0.54508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_estimators_array = [10, 50, 100]\n",
    "learning_rates = [0.1, 0.5, 1]\n",
    "\n",
    "# The base estimator is Desicion Tree Classifier\n",
    "for n_estimators in n_estimators_array:\n",
    "    for lr in learning_rates:\n",
    "        adc = AdaBoostClassifier(n_estimators = n_estimators, learning_rate = lr)\n",
    "        adc.fit(X_train, y_train)\n",
    "        print(f\"Number of estimators: {n_estimators}, Learning rate: {lr}, Accuracy: {adc.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46925489",
   "metadata": {},
   "source": [
    "Not much improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf0ae9",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d5af69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf, C: 0.01, Accuracy: 0.50432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf, C: 1, Accuracy: 0.50008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf, C: 10, Accuracy: 0.50604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: poly, C: 0.01, Accuracy: 0.49928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: poly, C: 1, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: poly, C: 10, Accuracy: 0.49352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.01, Accuracy: 0.48932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 1, Accuracy: 0.49668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 10, Accuracy: 0.50188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: sigmoid, C: 0.01, Accuracy: 0.50176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: sigmoid, C: 1, Accuracy: 0.49036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazat\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: sigmoid, C: 10, Accuracy: 0.488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['rbf', 'poly', 'linear' , 'sigmoid']\n",
    "cs = [0.01, 1, 10]\n",
    "\n",
    "# decision tree does not need scaled data to perform better\n",
    "# the same applies to all ensembling methods using desicion tree as base estimator\n",
    "for kernel in kernels:\n",
    "    for c in cs:\n",
    "        svc = SVC(kernel = kernel, C = c, max_iter = 500)\n",
    "        svc.fit(X_train_scaled, y_train)\n",
    "        print(f\"Kernel: {kernel}, C: {c}, Accuracy: {svc.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66eef3",
   "metadata": {},
   "source": [
    "Almost as bad as a random estimator, if not worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09788bd6",
   "metadata": {},
   "source": [
    "## Fully Connected NN without Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "df17eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d628b84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 20)                10020     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 30)                630       \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 30)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,681\n",
      "Trainable params: 10,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c7eb4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 19.6589 - accuracy: 0.4996\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.9715 - accuracy: 0.5009\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.7865 - accuracy: 0.5003\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.7275 - accuracy: 0.4977\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.7123 - accuracy: 0.4987\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.7035 - accuracy: 0.4987\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.7012 - accuracy: 0.4986\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6981 - accuracy: 0.4912\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.4952\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6976 - accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6938 - accuracy: 0.4987\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6937 - accuracy: 0.4982\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6954 - accuracy: 0.4976\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.4926\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5004\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.4962\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5012\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.4963\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.4957\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.4976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168845599a0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c3969d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 50.01%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a1ce9",
   "metadata": {},
   "source": [
    "It is unable to learn. Let's try with scaled input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "74afdd97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.6941 - accuracy: 0.5013\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6917 - accuracy: 0.5160\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6898 - accuracy: 0.5313\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5426\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6773 - accuracy: 0.5585\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6690 - accuracy: 0.5811\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6598 - accuracy: 0.5890\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6499 - accuracy: 0.6016\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 0s 1000us/step - loss: 0.6371 - accuracy: 0.6163\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6272 - accuracy: 0.6255\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6191 - accuracy: 0.6320\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6129 - accuracy: 0.6383\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.6464\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5974 - accuracy: 0.6556\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.6562\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.6556\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5850 - accuracy: 0.6660\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5796 - accuracy: 0.6630\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 0s 995us/step - loss: 0.5750 - accuracy: 0.6684\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.6757\n",
      "Model accuracy on the test dataset: 50.64%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_dim=500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "model_scores = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589999f",
   "metadata": {},
   "source": [
    "Perfoms better on training set, but it is still as bad as before on test set. We can conclude that th NN does not learn the dependencies between words, therefore it is biased on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c882d",
   "metadata": {},
   "source": [
    "# Using Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf36d4",
   "metadata": {},
   "source": [
    "## Fully Connected NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7dcb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the layers in the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# Define the layers in the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3be0eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 30)                480030    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,061\n",
      "Trainable params: 640,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b01efb0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5132 - accuracy: 0.7114\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2276 - accuracy: 0.9147\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0999 - accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168c63ea790>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbaebdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 86.90%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e0ee4",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67c50f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers in the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# Define the layers in the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(30, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba8a3592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 498, 30)           2910      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 249, 30)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 7470)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 30)                224130    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387,071\n",
      "Trainable params: 387,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e34832c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 7s 15ms/step - loss: 0.5047 - accuracy: 0.7251\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2800 - accuracy: 0.9037\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2261 - accuracy: 0.9294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168c8a46e20>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33804f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 88.04%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33c398",
   "metadata": {},
   "source": [
    "# RNN (using LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32f6d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how long the embedding vector will be\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# Define the layers in the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24874b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                4240      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,261\n",
      "Trainable params: 164,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff7b56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 39s 93ms/step - loss: 0.4941 - accuracy: 0.7539\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2768 - accuracy: 0.8884\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2627 - accuracy: 0.8933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168c9155ac0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "553f02d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 86.76%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90487b3f",
   "metadata": {},
   "source": [
    "## CNN-RNN(using GRU) Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76faef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers in the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# Define the layers in the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(30, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(GRU(20))\n",
    "model.add(Dense(20, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "702f225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_21 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 498, 30)           2910      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 249, 30)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 20)                3120      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,471\n",
      "Trainable params: 166,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba49012e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 28s 64ms/step - loss: 0.4598 - accuracy: 0.7652\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2554 - accuracy: 0.9009\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.1925 - accuracy: 0.9301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168d128d880>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca83eaac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 88.48%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d46c5e",
   "metadata": {},
   "source": [
    "## Bidirectional RNN with 2 stacked recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b0f98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(5, return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(5)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0696ede9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_24 (Embedding)    (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 500, 10)          1520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 10)               640       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 162,171\n",
      "Trainable params: 162,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dee4ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 84s 198ms/step - loss: 0.5138 - accuracy: 0.7652\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 0.3706 - accuracy: 0.8595\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 77s 197ms/step - loss: 0.2916 - accuracy: 0.8971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168d408f9d0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9f581081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the test dataset: 85.18%\n"
     ]
    }
   ],
   "source": [
    "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print out the accuracy of the model on the test set\n",
    "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f1e30",
   "metadata": {},
   "source": [
    "We can conclude that every NN performed much better than the previous models.The FNN is the fastest at training and evaluating while the Hybrid CNN-RNN has the best accuracy while is moderaly fast compared to RNN or Bidirectional RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
